{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6916d8aa",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63251e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f082267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch torchvision timm opencv-python-headless pillow matplotlib seaborn scikit-learn tqdm ultralytics -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cffae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "from ultralytics import YOLO\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3710090",
   "metadata": {},
   "source": [
    "## 2. Prepare Segmented Pod Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37c6ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8 segmentation model from previous training\n",
    "SEGMENTATION_MODEL_PATH = 'trained_models/cacao_segmentation_best.pt'\n",
    "\n",
    "if not Path(SEGMENTATION_MODEL_PATH).exists():\n",
    "    print(\"‚ö†Ô∏è Segmentation model not found. Please train YOLOv8 model first using train_cacao_segmentation_yolov8.ipynb\")\n",
    "    print(\"Or upload the trained model to 'trained_models/cacao_segmentation_best.pt'\")\n",
    "else:\n",
    "    seg_model = YOLO(SEGMENTATION_MODEL_PATH)\n",
    "    print(f\"‚úÖ Loaded segmentation model from {SEGMENTATION_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c24fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract segmented pods from images\n",
    "def extract_segmented_pods(image_dir, output_dir, seg_model, conf_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Extract individual pod crops from images using segmentation model\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    image_paths = list(Path(image_dir).glob('*.jpg')) + list(Path(image_dir).glob('*.png'))\n",
    "    print(f\"Processing {len(image_paths)} images...\")\n",
    "    \n",
    "    pod_count = 0\n",
    "    metadata = []\n",
    "    \n",
    "    for img_path in tqdm(image_paths):\n",
    "        # Run segmentation\n",
    "        results = seg_model.predict(str(img_path), conf=conf_threshold, verbose=False)\n",
    "        \n",
    "        if len(results) == 0 or results[0].masks is None:\n",
    "            continue\n",
    "        \n",
    "        # Load original image\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Extract each segmented pod\n",
    "        masks = results[0].masks.data.cpu().numpy()\n",
    "        boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "        \n",
    "        for idx, (mask, box) in enumerate(zip(masks, boxes)):\n",
    "            # Get bounding box\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            \n",
    "            # Resize mask to image size\n",
    "            mask_resized = cv2.resize(mask, (img.shape[1], img.shape[0]))\n",
    "            \n",
    "            # Apply mask to extract pod\n",
    "            masked_img = img_rgb.copy()\n",
    "            masked_img[mask_resized < 0.5] = 0  # Black background\n",
    "            \n",
    "            # Crop to bounding box\n",
    "            pod_crop = masked_img[y1:y2, x1:x2]\n",
    "            \n",
    "            # Calculate morphological features\n",
    "            area = np.sum(mask_resized > 0.5)\n",
    "            perimeter = cv2.arcLength(cv2.findContours(\n",
    "                (mask_resized > 0.5).astype(np.uint8),\n",
    "                cv2.RETR_EXTERNAL,\n",
    "                cv2.CHAIN_APPROX_SIMPLE\n",
    "            )[0][0], True)\n",
    "            \n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            aspect_ratio = width / height if height > 0 else 0\n",
    "            \n",
    "            # Save pod crop\n",
    "            pod_filename = f\"pod_{pod_count:05d}.jpg\"\n",
    "            pod_path = output_dir / pod_filename\n",
    "            Image.fromarray(pod_crop).save(pod_path)\n",
    "            \n",
    "            # Store metadata\n",
    "            metadata.append({\n",
    "                'pod_id': pod_count,\n",
    "                'filename': pod_filename,\n",
    "                'source_image': img_path.name,\n",
    "                'area': float(area),\n",
    "                'perimeter': float(perimeter),\n",
    "                'width': int(width),\n",
    "                'height': int(height),\n",
    "                'aspect_ratio': float(aspect_ratio),\n",
    "            })\n",
    "            \n",
    "            pod_count += 1\n",
    "    \n",
    "    # Save metadata\n",
    "    with open(output_dir / 'metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Extracted {pod_count} pods to {output_dir}\")\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3803db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pods from dataset\n",
    "# Assuming you have the cacao dataset from Roboflow\n",
    "DATASET_DIR = './cacao_dataset/train/images'  # Update this path\n",
    "SEGMENTED_PODS_DIR = './segmented_pods'\n",
    "\n",
    "if Path(SEGMENTATION_MODEL_PATH).exists():\n",
    "    metadata = extract_segmented_pods(DATASET_DIR, SEGMENTED_PODS_DIR, seg_model)\n",
    "    print(f\"Total pods extracted: {len(metadata)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping pod extraction - segmentation model not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa136d4",
   "metadata": {},
   "source": [
    "## 3. SimCLR Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01470546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimCLR augmentation pipeline\n",
    "class SimCLRAugmentation:\n",
    "    def __init__(self, img_size=224):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(img_size, scale=(0.2, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomApply([\n",
    "                transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "            ], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.transform(x), self.transform(x)  # Two augmented views\n",
    "\n",
    "# Standard transform for inference\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7237ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for SimCLR pre-training\n",
    "class CacaoPodDataset(Dataset):\n",
    "    def __init__(self, pod_dir, transform=None):\n",
    "        self.pod_dir = Path(pod_dir)\n",
    "        self.image_paths = list(self.pod_dir.glob('*.jpg'))\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            return self.transform(img)\n",
    "        else:\n",
    "            return test_transform(img)\n",
    "\n",
    "# Create dataset\n",
    "simclr_dataset = CacaoPodDataset(SEGMENTED_PODS_DIR, transform=SimCLRAugmentation())\n",
    "print(f\"Dataset size: {len(simclr_dataset)} pods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e5ada6",
   "metadata": {},
   "source": [
    "## 4. MobileNetV3 with SimCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cadd414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimCLR Model with MobileNetV3 backbone\n",
    "class SimCLRModel(nn.Module):\n",
    "    def __init__(self, base_model='mobilenetv3_large_100', projection_dim=128):\n",
    "        super(SimCLRModel, self).__init__()\n",
    "        \n",
    "        # Load MobileNetV3 from timm\n",
    "        self.encoder = timm.create_model(base_model, pretrained=True, num_classes=0)\n",
    "        \n",
    "        # Get feature dimension\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 224, 224)\n",
    "            feature_dim = self.encoder(dummy_input).shape[1]\n",
    "        \n",
    "        # Projection head for SimCLR\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim, projection_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        projections = self.projector(features)\n",
    "        return features, projections\n",
    "\n",
    "# NT-Xent Loss (Normalized Temperature-scaled Cross Entropy)\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, z_i, z_j):\n",
    "        batch_size = z_i.shape[0]\n",
    "        \n",
    "        # Normalize\n",
    "        z_i = F.normalize(z_i, dim=1)\n",
    "        z_j = F.normalize(z_j, dim=1)\n",
    "        \n",
    "        # Concatenate\n",
    "        representations = torch.cat([z_i, z_j], dim=0)\n",
    "        \n",
    "        # Similarity matrix\n",
    "        similarity_matrix = F.cosine_similarity(\n",
    "            representations.unsqueeze(1),\n",
    "            representations.unsqueeze(0),\n",
    "            dim=2\n",
    "        )\n",
    "        \n",
    "        # Create labels\n",
    "        labels = torch.cat([\n",
    "            torch.arange(batch_size) + batch_size,\n",
    "            torch.arange(batch_size)\n",
    "        ]).to(z_i.device)\n",
    "        \n",
    "        # Mask out self-similarity\n",
    "        mask = torch.eye(2 * batch_size, dtype=torch.bool).to(z_i.device)\n",
    "        similarity_matrix = similarity_matrix.masked_fill(mask, -9e15)\n",
    "        \n",
    "        # Compute loss\n",
    "        similarity_matrix = similarity_matrix / self.temperature\n",
    "        loss = F.cross_entropy(similarity_matrix, labels)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "model = SimCLRModel().to(device)\n",
    "print(f\"‚úÖ Model initialized on {device}\")\n",
    "print(f\"Encoder parameters: {sum(p.numel() for p in model.encoder.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058905c2",
   "metadata": {},
   "source": [
    "## 5. Train SimCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae42363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LR = 3e-4\n",
    "TEMPERATURE = 0.5\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    simclr_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "criterion = NTXentLoss(temperature=TEMPERATURE)\n",
    "\n",
    "print(f\"Training setup:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LR}\")\n",
    "print(f\"  Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4cf177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "history = {'loss': []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for (x_i, x_j) in pbar:\n",
    "        x_i, x_j = x_i.to(device), x_j.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        _, z_i = model(x_i)\n",
    "        _, z_j = model(x_j)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(z_i, z_j)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    history['loss'].append(avg_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, LR = {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    # Save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, f'simclr_checkpoint_epoch_{epoch+1}.pt')\n",
    "\n",
    "print(\"\\n‚úÖ SimCLR training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f48bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history['loss'])\n",
    "plt.title('SimCLR Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.savefig('simclr_training_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e1aa5",
   "metadata": {},
   "source": [
    "## 6. Extract Features and Cluster Pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d0191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from all pods\n",
    "def extract_features(model, dataset, batch_size=32):\n",
    "    model.eval()\n",
    "    features_list = []\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Extracting features\"):\n",
    "            if isinstance(batch, (list, tuple)):\n",
    "                batch = batch[0]  # Handle SimCLR augmentation\n",
    "            \n",
    "            batch = batch.to(device)\n",
    "            features, _ = model(batch)\n",
    "            features_list.append(features.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(features_list)\n",
    "\n",
    "# Create dataset without augmentation\n",
    "inference_dataset = CacaoPodDataset(SEGMENTED_PODS_DIR, transform=None)\n",
    "features = extract_features(model, inference_dataset)\n",
    "\n",
    "print(f\"\\n‚úÖ Extracted features: {features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f23d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster pods into Low/Medium/High yield categories\n",
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(features)\n",
    "\n",
    "# Map clusters to yield categories (based on cluster centers)\n",
    "# Assuming cluster with highest average feature values = high yield\n",
    "cluster_means = [features[cluster_labels == i].mean() for i in range(n_clusters)]\n",
    "cluster_ranking = np.argsort(cluster_means)  # Low to High\n",
    "\n",
    "yield_categories = ['Low', 'Medium', 'High']\n",
    "yield_mapping = {cluster_ranking[i]: yield_categories[i] for i in range(n_clusters)}\n",
    "\n",
    "yield_labels = [yield_mapping[label] for label in cluster_labels]\n",
    "\n",
    "print(\"\\nüìä Yield Distribution:\")\n",
    "for category in yield_categories:\n",
    "    count = yield_labels.count(category)\n",
    "    print(f\"  {category}: {count} pods ({count/len(yield_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fb1a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature space with PCA\n",
    "pca = PCA(n_components=2)\n",
    "features_2d = pca.fit_transform(features)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = {'Low': 'red', 'Medium': 'orange', 'High': 'green'}\n",
    "\n",
    "for category in yield_categories:\n",
    "    mask = np.array(yield_labels) == category\n",
    "    plt.scatter(\n",
    "        features_2d[mask, 0],\n",
    "        features_2d[mask, 1],\n",
    "        c=colors[category],\n",
    "        label=category,\n",
    "        alpha=0.6,\n",
    "        s=50\n",
    "    )\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Cacao Pod Feature Space (PCA)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('yield_clustering_pca.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPCA explained variance: {pca.explained_variance_ratio_.sum():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c8bbc",
   "metadata": {},
   "source": [
    "## 7. Siamese Ranking Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690d53f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese Ranking Head\n",
    "class YieldRankingModel(nn.Module):\n",
    "    def __init__(self, encoder, feature_dim=1280, hidden_dim=256):\n",
    "        super(YieldRankingModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        # Freeze encoder (use pre-trained features)\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Ranking head\n",
    "        self.ranking_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()  # Output: 0 = pod1 has higher yield, 1 = pod2 has higher yield\n",
    "        )\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            f1, _ = self.encoder(x1)\n",
    "            f2, _ = self.encoder(x2)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined = torch.cat([f1, f2], dim=1)\n",
    "        \n",
    "        # Predict ranking\n",
    "        score = self.ranking_head(combined)\n",
    "        \n",
    "        return score\n",
    "\n",
    "ranking_model = YieldRankingModel(model.encoder).to(device)\n",
    "print(f\"‚úÖ Ranking model initialized\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in ranking_model.parameters() if p.requires_grad) / 1e3:.2f}K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7934da5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic ranking pairs using cluster labels\n",
    "class RankingDataset(Dataset):\n",
    "    def __init__(self, pod_dir, cluster_labels, transform=None):\n",
    "        self.pod_dir = Path(pod_dir)\n",
    "        self.image_paths = sorted(list(self.pod_dir.glob('*.jpg')))\n",
    "        self.cluster_labels = cluster_labels\n",
    "        self.transform = transform if transform else test_transform\n",
    "        \n",
    "        # Create ranking pairs\n",
    "        self.pairs = []\n",
    "        for _ in range(len(self.image_paths) * 3):  # 3x data augmentation\n",
    "            idx1, idx2 = np.random.choice(len(self.image_paths), 2, replace=False)\n",
    "            \n",
    "            # Label: 1 if pod2 has higher yield, 0 otherwise\n",
    "            label = 1.0 if cluster_labels[idx2] > cluster_labels[idx1] else 0.0\n",
    "            \n",
    "            self.pairs.append((idx1, idx2, label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx1, idx2, label = self.pairs[idx]\n",
    "        \n",
    "        img1 = Image.open(self.image_paths[idx1]).convert('RGB')\n",
    "        img2 = Image.open(self.image_paths[idx2]).convert('RGB')\n",
    "        \n",
    "        return self.transform(img1), self.transform(img2), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "ranking_dataset = RankingDataset(SEGMENTED_PODS_DIR, cluster_labels)\n",
    "ranking_loader = DataLoader(ranking_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "print(f\"Ranking dataset: {len(ranking_dataset)} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b04498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ranking model\n",
    "ranking_optimizer = torch.optim.Adam(ranking_model.ranking_head.parameters(), lr=1e-3)\n",
    "ranking_criterion = nn.BCELoss()\n",
    "\n",
    "RANKING_EPOCHS = 20\n",
    "ranking_history = {'loss': [], 'accuracy': []}\n",
    "\n",
    "for epoch in range(RANKING_EPOCHS):\n",
    "    ranking_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(ranking_loader, desc=f\"Ranking Epoch {epoch+1}/{RANKING_EPOCHS}\")\n",
    "    for img1, img2, labels in pbar:\n",
    "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "        \n",
    "        ranking_optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        scores = ranking_model(img1, img2).squeeze()\n",
    "        loss = ranking_criterion(scores, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        ranking_optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predictions = (scores > 0.5).float()\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        pbar.set_postfix({'loss': loss.item(), 'acc': correct/total})\n",
    "    \n",
    "    avg_loss = epoch_loss / len(ranking_loader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    ranking_history['loss'].append(avg_loss)\n",
    "    ranking_history['accuracy'].append(accuracy)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Ranking model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead97113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ranking training\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(ranking_history['loss'])\n",
    "ax1.set_title('Ranking Model Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(ranking_history['accuracy'])\n",
    "ax2.set_title('Ranking Model Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ranking_training.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb6cf9",
   "metadata": {},
   "source": [
    "## 8. Export Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94910316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete model\n",
    "output_dir = Path('trained_models')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save SimCLR encoder\n",
    "torch.save({\n",
    "    'encoder_state_dict': model.encoder.state_dict(),\n",
    "    'projector_state_dict': model.projector.state_dict(),\n",
    "    'cluster_labels': cluster_labels,\n",
    "    'yield_mapping': yield_mapping,\n",
    "    'kmeans': kmeans,\n",
    "}, output_dir / 'simclr_encoder.pt')\n",
    "\n",
    "# Save ranking model\n",
    "torch.save({\n",
    "    'ranking_head_state_dict': ranking_model.ranking_head.state_dict(),\n",
    "    'accuracy': ranking_history['accuracy'][-1],\n",
    "}, output_dir / 'ranking_model.pt')\n",
    "\n",
    "# Save complete pipeline\n",
    "torch.save({\n",
    "    'encoder': model.encoder.state_dict(),\n",
    "    'ranking_head': ranking_model.ranking_head.state_dict(),\n",
    "    'yield_mapping': yield_mapping,\n",
    "    'cluster_centers': kmeans.cluster_centers_,\n",
    "}, output_dir / 'complete_yield_model.pt')\n",
    "\n",
    "print(\"\\n‚úÖ Models saved to trained_models/\")\n",
    "print(\"  - simclr_encoder.pt\")\n",
    "print(\"  - ranking_model.pt\")\n",
    "print(\"  - complete_yield_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad67a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX for mobile deployment\n",
    "ranking_model.eval()\n",
    "\n",
    "dummy_input1 = torch.randn(1, 3, 224, 224).to(device)\n",
    "dummy_input2 = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    ranking_model,\n",
    "    (dummy_input1, dummy_input2),\n",
    "    output_dir / 'yield_ranking_model.onnx',\n",
    "    input_names=['pod1', 'pod2'],\n",
    "    output_names=['ranking_score'],\n",
    "    dynamic_axes={\n",
    "        'pod1': {0: 'batch'},\n",
    "        'pod2': {0: 'batch'},\n",
    "        'ranking_score': {0: 'batch'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ ONNX model exported: yield_ranking_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41962306",
   "metadata": {},
   "source": [
    "## 9. Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49aa40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model info\n",
    "model_info = {\n",
    "    'model_name': 'Cacao Yield Estimation Pipeline',\n",
    "    'architecture': 'MobileNetV3 + SimCLR + Siamese Ranking',\n",
    "    'training_method': 'Self-supervised learning (no ground truth required)',\n",
    "    'num_pods_trained': len(simclr_dataset),\n",
    "    'simclr_epochs': EPOCHS,\n",
    "    'ranking_epochs': RANKING_EPOCHS,\n",
    "    'ranking_accuracy': float(ranking_history['accuracy'][-1]),\n",
    "    'yield_categories': ['Low', 'Medium', 'High'],\n",
    "    'input_size': [224, 224],\n",
    "    'feature_dim': features.shape[1],\n",
    "    'deployment_formats': ['PyTorch', 'ONNX'],\n",
    "}\n",
    "\n",
    "with open(output_dir / 'yield_model_info.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(\"\\nüìä Model Information:\")\n",
    "for key, value in model_info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n‚úÖ Model info saved to: trained_models/yield_model_info.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6911d778",
   "metadata": {},
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "### What We Built:\n",
    "1. **YOLOv8 Segmentation**: Detects and segments cacao pods (trained in previous notebook)\n",
    "2. **SimCLR Encoder**: Learns pod features without labels using self-supervised learning\n",
    "3. **Siamese Ranking**: Compares two pods and predicts relative yield\n",
    "4. **Clustering**: Groups pods into Low/Medium/High yield categories\n",
    "\n",
    "### Deployment Pipeline:\n",
    "```\n",
    "Input Image ‚Üí YOLOv8 Segmentation ‚Üí Pod Crops ‚Üí SimCLR Features ‚Üí \n",
    "Ranking Model ‚Üí Yield Prediction (Low/Medium/High)\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "1. Download models from `trained_models/` directory\n",
    "2. Integrate into mobile app\n",
    "3. Test on real field images\n",
    "4. Fine-tune with farmer feedback\n",
    "\n",
    "### Model Files:\n",
    "- `cacao_segmentation_best.pt` - YOLOv8 segmentation model\n",
    "- `complete_yield_model.pt` - Full yield estimation pipeline\n",
    "- `yield_ranking_model.onnx` - ONNX format for mobile deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c38edae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically download trained models to your computer\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if running in Google Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running in Google Colab - Will auto-download models\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üíª Not in Colab - Models saved locally\")\n",
    "\n",
    "# List of models to download\n",
    "models_to_download = [\n",
    "    'trained_models/simclr_encoder.pt',\n",
    "    'trained_models/ranking_model.pt',\n",
    "    'trained_models/complete_yield_model.pt',\n",
    "    'trained_models/yield_ranking_model.onnx',\n",
    "    'trained_models/model_info.json',\n",
    "    'trained_models/yield_model_info.json',\n",
    "]\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\\nüì• Downloading models to your computer...\")\n",
    "    downloaded_count = 0\n",
    "    for model_path in models_to_download:\n",
    "        if os.path.exists(model_path):\n",
    "            file_size = os.path.getsize(model_path) / 1e6\n",
    "            print(f\"\\n‚¨áÔ∏è Downloading: {Path(model_path).name} ({file_size:.2f} MB)\")\n",
    "            try:\n",
    "                files.download(model_path)\n",
    "                print(f\"   ‚úÖ Downloaded successfully!\")\n",
    "                downloaded_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Download failed: {e}\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è Model not found: {model_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üéâ {downloaded_count} models downloaded to your Downloads folder!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nüìå Important files:\")\n",
    "    print(\"   ‚Ä¢ yield_ranking_model.onnx - For mobile app\")\n",
    "    print(\"   ‚Ä¢ complete_yield_model.pt - Full pipeline\")\n",
    "    print(\"   ‚Ä¢ yield_model_info.json - Model metadata\")\n",
    "else:\n",
    "    print(\"\\nüìÇ Models saved locally at:\")\n",
    "    for model_path in models_to_download:\n",
    "        if os.path.exists(model_path):\n",
    "            file_size = os.path.getsize(model_path) / 1e6\n",
    "            print(f\"  ‚úÖ {model_path} ({file_size:.2f} MB)\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå {model_path} (not found)\")\n",
    "    \n",
    "    print(\"\\nüí° To use in your mobile app:\")\n",
    "    print(\"   1. Copy yield_ranking_model.onnx to: mobile-app/assets/models/\")\n",
    "    print(\"   2. Copy complete_yield_model.pt to: public/models/\")\n",
    "    print(\"\\nüìñ Next steps:\")\n",
    "    print(\"   1. Download cacao_segmentation_best.onnx from previous notebook\")\n",
    "    print(\"   2. Add both ONNX models to mobile app\")\n",
    "    print(\"   3. Run: cd mobile-app && npx expo start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8c9351",
   "metadata": {},
   "source": [
    "## 10. Auto-Download Models (Google Colab)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
