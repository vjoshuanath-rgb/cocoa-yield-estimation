{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5506470b",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06688239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"cuDNN Version: {torch.backends.cudnn.version()}\")\n",
    "    \n",
    "    # Check if A100\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if 'A100' in gpu_name:\n",
    "        print(\"\\n‚úÖ A100 GPU detected! Optimizations enabled.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Warning: Expected A100, but found {gpu_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611cd443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch torchvision timm opencv-python-headless pillow matplotlib seaborn scikit-learn tqdm ultralytics -q\n",
    "\n",
    "print(\"‚úÖ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1df628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "from ultralytics import YOLO\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Enable cuDNN benchmarking for A100\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"AMP (Mixed Precision) enabled: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55076f3a",
   "metadata": {},
   "source": [
    "## 2. Prepare Segmented Pod Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b412572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8 segmentation model from previous training\n",
    "SEGMENTATION_MODEL_PATH = 'trained_models/cacao_segmentation_best.pt'\n",
    "\n",
    "if not Path(SEGMENTATION_MODEL_PATH).exists():\n",
    "    print(\"‚ö†Ô∏è Segmentation model not found. Please train YOLOv8 model first using train_cacao_segmentation_yolov8.ipynb\")\n",
    "    print(\"Or upload the trained model to 'trained_models/cacao_segmentation_best.pt'\")\n",
    "else:\n",
    "    seg_model = YOLO(SEGMENTATION_MODEL_PATH)\n",
    "    print(f\"‚úÖ Loaded segmentation model from {SEGMENTATION_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a798210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract segmented pods from images\n",
    "def extract_segmented_pods(image_dir, output_dir, seg_model, conf_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Extract individual pod crops from images using segmentation model\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    image_paths = list(Path(image_dir).glob('*.jpg')) + list(Path(image_dir).glob('*.png'))\n",
    "    print(f\"Processing {len(image_paths)} images...\")\n",
    "    \n",
    "    pod_count = 0\n",
    "    metadata = []\n",
    "    \n",
    "    for img_path in tqdm(image_paths):\n",
    "        # Run segmentation\n",
    "        results = seg_model.predict(str(img_path), conf=conf_threshold, verbose=False)\n",
    "        \n",
    "        if len(results) == 0 or results[0].masks is None:\n",
    "            continue\n",
    "        \n",
    "        # Load original image\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Extract each segmented pod\n",
    "        masks = results[0].masks.data.cpu().numpy()\n",
    "        boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "        \n",
    "        for idx, (mask, box) in enumerate(zip(masks, boxes)):\n",
    "            # Get bounding box\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            \n",
    "            # Resize mask to image size\n",
    "            mask_resized = cv2.resize(mask, (img.shape[1], img.shape[0]))\n",
    "            \n",
    "            # Apply mask to extract pod\n",
    "            masked_img = img_rgb.copy()\n",
    "            masked_img[mask_resized < 0.5] = 0  # Black background\n",
    "            \n",
    "            # Crop to bounding box\n",
    "            pod_crop = masked_img[y1:y2, x1:x2]\n",
    "            \n",
    "            # Calculate morphological features\n",
    "            area = np.sum(mask_resized > 0.5)\n",
    "            perimeter = cv2.arcLength(cv2.findContours(\n",
    "                (mask_resized > 0.5).astype(np.uint8),\n",
    "                cv2.RETR_EXTERNAL,\n",
    "                cv2.CHAIN_APPROX_SIMPLE\n",
    "            )[0][0], True)\n",
    "            \n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            aspect_ratio = width / height if height > 0 else 0\n",
    "            \n",
    "            # Save pod crop\n",
    "            pod_filename = f\"pod_{pod_count:05d}.jpg\"\n",
    "            pod_path = output_dir / pod_filename\n",
    "            Image.fromarray(pod_crop).save(pod_path)\n",
    "            \n",
    "            # Store metadata\n",
    "            metadata.append({\n",
    "                'pod_id': pod_count,\n",
    "                'filename': pod_filename,\n",
    "                'source_image': img_path.name,\n",
    "                'area': float(area),\n",
    "                'perimeter': float(perimeter),\n",
    "                'width': int(width),\n",
    "                'height': int(height),\n",
    "                'aspect_ratio': float(aspect_ratio),\n",
    "            })\n",
    "            \n",
    "            pod_count += 1\n",
    "    \n",
    "    # Save metadata\n",
    "    with open(output_dir / 'metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Extracted {pod_count} pods to {output_dir}\")\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d2fea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Cacao Dataset from Roboflow\n",
    "!pip install roboflow -q\n",
    "\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# Initialize Roboflow (you'll need to add your API key)\n",
    "rf = Roboflow(api_key=\"BmOmRgtqhSUKBitTttWj\")  # Replace with your actual API key\n",
    "project = rf.workspace(\"cariesdetectionproject\").project(\"cacao-uf6rm\")\n",
    "dataset = project.version(5).download(\"yolov8\")\n",
    "\n",
    "print(f\"‚úÖ Dataset downloaded to: {dataset.location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pods from dataset\n",
    "# Use the actual downloaded dataset location\n",
    "DATASET_DIR = './Cacao-2/train/images'  # Roboflow downloads to Cacao-2\n",
    "SEGMENTED_PODS_DIR = './segmented_pods'\n",
    "\n",
    "if Path(SEGMENTATION_MODEL_PATH).exists():\n",
    "    metadata = extract_segmented_pods(DATASET_DIR, SEGMENTED_PODS_DIR, seg_model)\n",
    "    print(f\"Total pods extracted: {len(metadata)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping pod extraction - segmentation model not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad38ba",
   "metadata": {},
   "source": [
    "## 3. SimCLR Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a0a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimCLR augmentation pipeline\n",
    "class SimCLRAugmentation:\n",
    "    def __init__(self, img_size=224):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(img_size, scale=(0.2, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomApply([\n",
    "                transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "            ], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.transform(x), self.transform(x)  # Two augmented views\n",
    "\n",
    "# Standard transform for inference\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e97368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for SimCLR pre-training\n",
    "class CacaoPodDataset(Dataset):\n",
    "    def __init__(self, pod_dir, transform=None):\n",
    "        self.pod_dir = Path(pod_dir)\n",
    "        self.image_paths = list(self.pod_dir.glob('*.jpg'))\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            return self.transform(img)\n",
    "        else:\n",
    "            return test_transform(img)\n",
    "\n",
    "# Create dataset\n",
    "simclr_dataset = CacaoPodDataset(SEGMENTED_PODS_DIR, transform=SimCLRAugmentation())\n",
    "print(f\"Dataset size: {len(simclr_dataset)} pods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d257aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç DEBUG: Check if pods were extracted\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "pods_found = list(Path(SEGMENTED_PODS_DIR).glob('*.jpg'))\n",
    "print(f\"‚úÖ Found {len(pods_found)} pods in {SEGMENTED_PODS_DIR}\")\n",
    "\n",
    "if len(pods_found) == 0:\n",
    "    print(\"‚ùå ERROR: No pods found!\")\n",
    "    print(\"   Make sure you:\")\n",
    "    print(\"   1. Ran cells 2-8 to extract pods\")\n",
    "    print(\"   2. Uploaded your segmentation model\")\n",
    "    print(\"   3. Dataset images are available\")\n",
    "else:\n",
    "    print(f\"üì∏ Sample pods: {pods_found[:3]}\")\n",
    "    print(\"‚úÖ Ready to create dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dec080",
   "metadata": {},
   "source": [
    "## 4. MobileNetV3 with SimCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498903db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimCLR Model with MobileNetV3 backbone\n",
    "class SimCLRModel(nn.Module):\n",
    "    def __init__(self, base_model='mobilenetv3_large_100', projection_dim=128):\n",
    "        super(SimCLRModel, self).__init__()\n",
    "        \n",
    "        # Load MobileNetV3 from timm\n",
    "        self.encoder = timm.create_model(base_model, pretrained=True, num_classes=0)\n",
    "        \n",
    "        # Get feature dimension\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 224, 224)\n",
    "            feature_dim = self.encoder(dummy_input).shape[1]\n",
    "        \n",
    "        # Projection head for SimCLR\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim, projection_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        projections = self.projector(features)\n",
    "        return features, projections\n",
    "\n",
    "# NT-Xent Loss (Normalized Temperature-scaled Cross Entropy)\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, z_i, z_j):\n",
    "        batch_size = z_i.shape[0]\n",
    "        \n",
    "        # Normalize\n",
    "        z_i = F.normalize(z_i, dim=1)\n",
    "        z_j = F.normalize(z_j, dim=1)\n",
    "        \n",
    "        # Concatenate\n",
    "        representations = torch.cat([z_i, z_j], dim=0)\n",
    "        \n",
    "        # Similarity matrix\n",
    "        similarity_matrix = F.cosine_similarity(\n",
    "            representations.unsqueeze(1),\n",
    "            representations.unsqueeze(0),\n",
    "            dim=2\n",
    "        )\n",
    "        \n",
    "        # Create labels\n",
    "        labels = torch.cat([\n",
    "            torch.arange(batch_size) + batch_size,\n",
    "            torch.arange(batch_size)\n",
    "        ]).to(z_i.device)\n",
    "        \n",
    "        # Mask out self-similarity\n",
    "        mask = torch.eye(2 * batch_size, dtype=torch.bool).to(z_i.device)\n",
    "        similarity_matrix = similarity_matrix.masked_fill(mask, -9e15)\n",
    "        \n",
    "        # Compute loss\n",
    "        similarity_matrix = similarity_matrix / self.temperature\n",
    "        loss = F.cross_entropy(similarity_matrix, labels)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "model = SimCLRModel().to(device)\n",
    "print(f\"‚úÖ Model initialized on {device}\")\n",
    "print(f\"Encoder parameters: {sum(p.numel() for p in model.encoder.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d8a66",
   "metadata": {},
   "source": [
    "## 5. Train SimCLR (A100 Optimized with Mixed Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e38ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration - A100 optimized\n",
    "BATCH_SIZE = 256  # Increased from 64 for A100's 40GB VRAM\n",
    "EPOCHS = 100\n",
    "LR = 3e-4\n",
    "TEMPERATURE = 0.5\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    simclr_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=8,  # Increased from 4 for A100\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    persistent_workers=True  # Keep workers alive between epochs\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "criterion = NTXentLoss(temperature=TEMPERATURE)\n",
    "\n",
    "# Initialize gradient scaler for mixed precision\n",
    "scaler = GradScaler()\n",
    "\n",
    "print(f\"Training setup (A100 Optimized):\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LR}\")\n",
    "print(f\"  Batches per epoch: {len(train_loader)}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Mixed Precision (AMP): Enabled\")\n",
    "print(f\"  Workers: 8 (persistent)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91b324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with A100 optimizations (Mixed Precision)\n",
    "history = {'loss': []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for (x_i, x_j) in pbar:\n",
    "        x_i, x_j = x_i.to(device), x_j.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        with autocast():\n",
    "            _, z_i = model(x_i)\n",
    "            _, z_j = model(x_j)\n",
    "            loss = criterion(z_i, z_j)\n",
    "        \n",
    "        # Mixed precision backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    history['loss'].append(avg_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, LR = {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    # Save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, f'simclr_checkpoint_epoch_{epoch+1}_a100.pt')\n",
    "\n",
    "print(\"\\n‚úÖ SimCLR training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79741f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history['loss'])\n",
    "plt.title('SimCLR Training Loss (A100 GPU)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.savefig('simclr_training_loss_a100.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368410c5",
   "metadata": {},
   "source": [
    "## 6. Extract Features and Cluster Pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d0cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from all pods (A100 optimized)\n",
    "def extract_features(model, dataset, batch_size=64):\n",
    "    model.eval()\n",
    "    features_list = []\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Extracting features\"):\n",
    "            if isinstance(batch, (list, tuple)):\n",
    "                batch = batch[0]  # Handle SimCLR augmentation\n",
    "            \n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Use mixed precision for inference\n",
    "            with autocast():\n",
    "                features, _ = model(batch)\n",
    "            \n",
    "            features_list.append(features.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(features_list)\n",
    "\n",
    "# Create dataset without augmentation\n",
    "inference_dataset = CacaoPodDataset(SEGMENTED_PODS_DIR, transform=None)\n",
    "features = extract_features(model, inference_dataset)\n",
    "\n",
    "print(f\"\\n‚úÖ Extracted features: {features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577002d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster pods into Low/Medium/High yield categories\n",
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(features)\n",
    "\n",
    "# Map clusters to yield categories (based on cluster centers)\n",
    "# Assuming cluster with highest average feature values = high yield\n",
    "cluster_means = [features[cluster_labels == i].mean() for i in range(n_clusters)]\n",
    "cluster_ranking = np.argsort(cluster_means)  # Low to High\n",
    "\n",
    "yield_categories = ['Low', 'Medium', 'High']\n",
    "yield_mapping = {cluster_ranking[i]: yield_categories[i] for i in range(n_clusters)}\n",
    "\n",
    "yield_labels = [yield_mapping[label] for label in cluster_labels]\n",
    "\n",
    "print(\"\\nüìä Yield Distribution:\")\n",
    "for category in yield_categories:\n",
    "    count = yield_labels.count(category)\n",
    "    print(f\"  {category}: {count} pods ({count/len(yield_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a7028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature space with PCA\n",
    "pca = PCA(n_components=2)\n",
    "features_2d = pca.fit_transform(features)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = {'Low': 'red', 'Medium': 'orange', 'High': 'green'}\n",
    "\n",
    "for category in yield_categories:\n",
    "    mask = np.array(yield_labels) == category\n",
    "    plt.scatter(\n",
    "        features_2d[mask, 0],\n",
    "        features_2d[mask, 1],\n",
    "        c=colors[category],\n",
    "        label=category,\n",
    "        alpha=0.6,\n",
    "        s=50\n",
    "    )\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Cacao Pod Feature Space (PCA) - A100 Trained')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('yield_clustering_pca_a100.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPCA explained variance: {pca.explained_variance_ratio_.sum():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adddeb1c",
   "metadata": {},
   "source": [
    "## 7. Siamese Ranking Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4a9d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese Ranking Head\n",
    "class YieldRankingModel(nn.Module):\n",
    "    def __init__(self, encoder, feature_dim=1280, hidden_dim=256):\n",
    "        super(YieldRankingModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        # Freeze encoder (use pre-trained features)\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Ranking head\n",
    "        self.ranking_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()  # Output: 0 = pod1 has higher yield, 1 = pod2 has higher yield\n",
    "        )\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            f1, _ = self.encoder(x1)\n",
    "            f2, _ = self.encoder(x2)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined = torch.cat([f1, f2], dim=1)\n",
    "        \n",
    "        # Predict ranking\n",
    "        score = self.ranking_head(combined)\n",
    "        \n",
    "        return score\n",
    "\n",
    "ranking_model = YieldRankingModel(model.encoder).to(device)\n",
    "print(f\"‚úÖ Ranking model initialized on {device}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in ranking_model.parameters() if p.requires_grad) / 1e3:.2f}K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61162e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic ranking pairs using cluster labels\n",
    "class RankingDataset(Dataset):\n",
    "    def __init__(self, pod_dir, cluster_labels, transform=None):\n",
    "        self.pod_dir = Path(pod_dir)\n",
    "        self.image_paths = sorted(list(self.pod_dir.glob('*.jpg')))\n",
    "        self.cluster_labels = cluster_labels\n",
    "        self.transform = transform if transform else test_transform\n",
    "        \n",
    "        # Create ranking pairs\n",
    "        self.pairs = []\n",
    "        for _ in range(len(self.image_paths) * 3):  # 3x data augmentation\n",
    "            idx1, idx2 = np.random.choice(len(self.image_paths), 2, replace=False)\n",
    "            \n",
    "            # Label: 1 if pod2 has higher yield, 0 otherwise\n",
    "            label = 1.0 if cluster_labels[idx2] > cluster_labels[idx1] else 0.0\n",
    "            \n",
    "            self.pairs.append((idx1, idx2, label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx1, idx2, label = self.pairs[idx]\n",
    "        \n",
    "        img1 = Image.open(self.image_paths[idx1]).convert('RGB')\n",
    "        img2 = Image.open(self.image_paths[idx2]).convert('RGB')\n",
    "        \n",
    "        return self.transform(img1), self.transform(img2), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "ranking_dataset = RankingDataset(SEGMENTED_PODS_DIR, cluster_labels)\n",
    "ranking_loader = DataLoader(\n",
    "    ranking_dataset, \n",
    "    batch_size=64,  # Increased for A100\n",
    "    shuffle=True, \n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "print(f\"Ranking dataset: {len(ranking_dataset)} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f4fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ranking model (A100 optimized with mixed precision)\n",
    "ranking_optimizer = torch.optim.Adam(ranking_model.ranking_head.parameters(), lr=1e-3)\n",
    "ranking_criterion = nn.BCELoss()\n",
    "ranking_scaler = GradScaler()\n",
    "\n",
    "RANKING_EPOCHS = 20\n",
    "ranking_history = {'loss': [], 'accuracy': []}\n",
    "\n",
    "for epoch in range(RANKING_EPOCHS):\n",
    "    ranking_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(ranking_loader, desc=f\"Ranking Epoch {epoch+1}/{RANKING_EPOCHS}\")\n",
    "    for img1, img2, labels in pbar:\n",
    "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "        \n",
    "        ranking_optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        with autocast():\n",
    "            scores = ranking_model(img1, img2).squeeze()\n",
    "            loss = ranking_criterion(scores, labels)\n",
    "        \n",
    "        # Mixed precision backward pass\n",
    "        ranking_scaler.scale(loss).backward()\n",
    "        ranking_scaler.step(ranking_optimizer)\n",
    "        ranking_scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predictions = (scores > 0.5).float()\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        pbar.set_postfix({'loss': loss.item(), 'acc': correct/total})\n",
    "    \n",
    "    avg_loss = epoch_loss / len(ranking_loader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    ranking_history['loss'].append(avg_loss)\n",
    "    ranking_history['accuracy'].append(accuracy)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Ranking model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c5f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ranking training\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(ranking_history['loss'])\n",
    "ax1.set_title('Ranking Model Loss (A100)')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(ranking_history['accuracy'])\n",
    "ax2.set_title('Ranking Model Accuracy (A100)')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ranking_training_a100.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b68223c",
   "metadata": {},
   "source": [
    "## 8. Export Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f14b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete model\n",
    "output_dir = Path('trained_models')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save SimCLR encoder\n",
    "torch.save({\n",
    "    'encoder_state_dict': model.encoder.state_dict(),\n",
    "    'projector_state_dict': model.projector.state_dict(),\n",
    "    'cluster_labels': cluster_labels,\n",
    "    'yield_mapping': yield_mapping,\n",
    "    'kmeans': kmeans,\n",
    "}, output_dir / 'simclr_encoder_a100.pt')\n",
    "\n",
    "# Save ranking model\n",
    "torch.save({\n",
    "    'ranking_head_state_dict': ranking_model.ranking_head.state_dict(),\n",
    "    'accuracy': ranking_history['accuracy'][-1],\n",
    "}, output_dir / 'ranking_model_a100.pt')\n",
    "\n",
    "# Save complete pipeline\n",
    "torch.save({\n",
    "    'encoder': model.encoder.state_dict(),\n",
    "    'ranking_head': ranking_model.ranking_head.state_dict(),\n",
    "    'yield_mapping': yield_mapping,\n",
    "    'cluster_centers': kmeans.cluster_centers_,\n",
    "}, output_dir / 'complete_yield_model_a100.pt')\n",
    "\n",
    "print(\"\\n‚úÖ Models saved to trained_models/ (A100 trained)\")\n",
    "print(\"  - simclr_encoder_a100.pt\")\n",
    "print(\"  - ranking_model_a100.pt\")\n",
    "print(\"  - complete_yield_model_a100.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14dcf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX for mobile deployment\n",
    "ranking_model.eval()\n",
    "\n",
    "# Move to CPU for ONNX export\n",
    "ranking_model_cpu = ranking_model.cpu()\n",
    "dummy_input1 = torch.randn(1, 3, 224, 224)\n",
    "dummy_input2 = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "torch.onnx.export(\n",
    "    ranking_model_cpu,\n",
    "    (dummy_input1, dummy_input2),\n",
    "    output_dir / 'yield_ranking_model_a100.onnx',\n",
    "    input_names=['pod1', 'pod2'],\n",
    "    output_names=['ranking_score'],\n",
    "    dynamic_axes={\n",
    "        'pod1': {0: 'batch'},\n",
    "        'pod2': {0: 'batch'},\n",
    "        'ranking_score': {0: 'batch'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ ONNX model exported: yield_ranking_model_a100.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8c2fb",
   "metadata": {},
   "source": [
    "## 9. Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a663b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model info\n",
    "model_info = {\n",
    "    'model_name': 'Cacao Yield Estimation Pipeline (A100 Trained)',\n",
    "    'architecture': 'MobileNetV3 + SimCLR + Siamese Ranking',\n",
    "    'training_method': 'Self-supervised learning (no ground truth required)',\n",
    "    'hardware': 'NVIDIA A100 GPU',\n",
    "    'mixed_precision': 'FP16 (AMP enabled)',\n",
    "    'num_pods_trained': len(simclr_dataset),\n",
    "    'simclr_epochs': EPOCHS,\n",
    "    'ranking_epochs': RANKING_EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'ranking_accuracy': float(ranking_history['accuracy'][-1]),\n",
    "    'yield_categories': ['Low', 'Medium', 'High'],\n",
    "    'input_size': [224, 224],\n",
    "    'feature_dim': features.shape[1],\n",
    "    'deployment_formats': ['PyTorch', 'ONNX'],\n",
    "}\n",
    "\n",
    "with open(output_dir / 'yield_model_info_a100.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(\"\\nüìä Model Information (A100 Trained):\")\n",
    "for key, value in model_info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n‚úÖ Model info saved to: trained_models/yield_model_info_a100.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e9999",
   "metadata": {},
   "source": [
    "## üéâ A100 Training Complete!\n",
    "\n",
    "### A100 Performance Benefits:\n",
    "- **4-6x faster training** vs T4 GPU\n",
    "- **Larger batch size** (256 vs 64) for superior convergence\n",
    "- **Mixed Precision (FP16)** for 2x memory efficiency\n",
    "- **40GB VRAM** enables massive batch sizes\n",
    "- **Persistent workers** reduce data loading overhead\n",
    "- **cuDNN benchmarking** for optimal kernel selection\n",
    "\n",
    "### Performance Comparison:\n",
    "| Hardware | Batch Size | Speed | Memory |\n",
    "|----------|-----------|-------|--------|\n",
    "| T4 GPU   | 64        | 1x    | 16GB   |\n",
    "| TPU v5e  | 128       | 3-5x  | -      |\n",
    "| **A100** | **256**   | **4-6x** | **40GB** |\n",
    "\n",
    "### What We Built:\n",
    "1. **YOLOv8 Segmentation**: Detects and segments cacao pods\n",
    "2. **SimCLR Encoder**: Learns pod features without labels using self-supervised learning\n",
    "3. **Siamese Ranking**: Compares two pods and predicts relative yield\n",
    "4. **Clustering**: Groups pods into Low/Medium/High yield categories\n",
    "\n",
    "### Deployment Pipeline:\n",
    "```\n",
    "Input Image ‚Üí YOLOv8 Segmentation ‚Üí Pod Crops ‚Üí SimCLR Features ‚Üí \n",
    "Ranking Model ‚Üí Yield Prediction (Low/Medium/High)\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "1. Download models from `trained_models/` directory\n",
    "2. Integrate into mobile app\n",
    "3. Test on real field images\n",
    "4. Fine-tune with farmer feedback\n",
    "\n",
    "### Model Files (A100 Trained):\n",
    "- `simclr_encoder_a100.pt` - SimCLR encoder with FP16 training\n",
    "- `ranking_model_a100.pt` - Ranking model with mixed precision\n",
    "- `complete_yield_model_a100.pt` - Full pipeline\n",
    "- `yield_ranking_model_a100.onnx` - ONNX format for mobile deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22274929",
   "metadata": {},
   "source": [
    "## 10. Auto-Download Models (Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e22896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically download trained models to your computer\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if running in Google Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running in Google Colab - Will auto-download models\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üíª Not in Colab - Models saved locally\")\n",
    "\n",
    "# List of models to download\n",
    "models_to_download = [\n",
    "    'trained_models/simclr_encoder_a100.pt',\n",
    "    'trained_models/ranking_model_a100.pt',\n",
    "    'trained_models/complete_yield_model_a100.pt',\n",
    "    'trained_models/yield_ranking_model_a100.onnx',\n",
    "    'trained_models/yield_model_info_a100.json',\n",
    "]\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\\nüì• Downloading A100-trained models to your computer...\")\n",
    "    downloaded_count = 0\n",
    "    for model_path in models_to_download:\n",
    "        if os.path.exists(model_path):\n",
    "            file_size = os.path.getsize(model_path) / 1e6\n",
    "            print(f\"\\n‚¨áÔ∏è Downloading: {Path(model_path).name} ({file_size:.2f} MB)\")\n",
    "            try:\n",
    "                files.download(model_path)\n",
    "                print(f\"   ‚úÖ Downloaded successfully!\")\n",
    "                downloaded_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Download failed: {e}\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è Model not found: {model_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üéâ {downloaded_count} A100-trained models downloaded to your Downloads folder!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nüìå Important files:\")\n",
    "    print(\"   ‚Ä¢ yield_ranking_model_a100.onnx - For mobile app\")\n",
    "    print(\"   ‚Ä¢ complete_yield_model_a100.pt - Full pipeline\")\n",
    "    print(\"   ‚Ä¢ yield_model_info_a100.json - Model metadata\")\n",
    "    print(\"\\n‚ö° A100 Performance Benefits:\")\n",
    "    print(\"   ‚Ä¢ 4-6x faster training vs T4 GPU\")\n",
    "    print(\"   ‚Ä¢ Larger batch size (256) for better convergence\")\n",
    "    print(\"   ‚Ä¢ Mixed Precision (FP16) for 2x memory efficiency\")\n",
    "    print(\"   ‚Ä¢ 40GB VRAM for maximum throughput\")\n",
    "else:\n",
    "    print(\"\\nüìÇ A100-trained models saved locally at:\")\n",
    "    for model_path in models_to_download:\n",
    "        if os.path.exists(model_path):\n",
    "            file_size = os.path.getsize(model_path) / 1e6\n",
    "            print(f\"  ‚úÖ {model_path} ({file_size:.2f} MB)\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå {model_path} (not found)\")\n",
    "    \n",
    "    print(\"\\nüí° To use in your mobile app:\")\n",
    "    print(\"   1. Copy yield_ranking_model_a100.onnx to: mobile-app/assets/models/\")\n",
    "    print(\"   2. Copy complete_yield_model_a100.pt to: public/models/\")\n",
    "    print(\"\\nüìñ Next steps:\")\n",
    "    print(\"   1. Download cacao_segmentation_best.onnx from previous notebook\")\n",
    "    print(\"   2. Add both ONNX models to mobile app\")\n",
    "    print(\"   3. Run: cd mobile-app && npx expo start\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
